---
layout: post
title: "WavCaps"
date: 2023-03-30
project_url: https://github.com/XinhaoMei/WavCaps
institutions:
- Centre for Vision, Speech, and Signal Processing (CVSSP), University of Surrey
- Johns Hopkins University
- ByteDance
- School of Electronic and Computer Engineering, Peking University
authors: 
- Xinhao Mei
- Chutong Meng
- Haohe Liu
- Qiuqiang Kong
- Tom Ko
- Chengqi Zhao
- Mark D. Plumbley
- Yuexian Zou
- Wenwu Wang
---
 
WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset (with 260k clips sourced from Freesound website). However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically.


